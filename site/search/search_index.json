{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"Linux/","text":"Linux.markdown\u76ee\u7684\u5728\u4e8e\u7b80\u5355\u8bb0\u5f55\u4e00\u4e9bLinux\u57fa\u672c\u77e5\u8bc6\u70b9\u65b9\u4fbf\u540e\u7eed\u56de\u987e\uff08\u5185\u5bb9\u6765\u81ea\u7f51\u7edc\uff09 Linux\u7cfb\u7edf\u76ee\u5f55\uff08\u5f85\u7ec6\u5316\uff09 bin \u5b58\u653e\u4e8c\u8fdb\u5236\u53ef\u6267\u884c\u6587\u4ef6(ls,cat,mkdir\u7b49) boot \u5b58\u653e\u7528\u4e8e\u7cfb\u7edf\u5f15\u5bfc\u65f6\u4f7f\u7528\u7684\u5404\u79cd\u6587\u4ef6 dev \u7528\u4e8e\u5b58\u653e\u8bbe\u5907\u6587\u4ef6 etc \u5b58\u653e\u7cfb\u7edf\u914d\u7f6e\u6587\u4ef6 home \u5b58\u653e\u6240\u6709\u7528\u6237\u6587\u4ef6\u7684\u6839\u76ee\u5f55 lib \u5b58\u653e\u8ddf\u6587\u4ef6\u7cfb\u7edf\u4e2d\u7684\u7a0b\u5e8f\u8fd0\u884c\u6240\u9700\u8981\u7684\u5171\u4eab\u5e93\u53ca\u5185\u6838\u6a21\u5757 mnt \u7cfb\u7edf\u7ba1\u7406\u5458\u5b89\u88c5\u4e34\u65f6\u6587\u4ef6\u7cfb\u7edf\u7684\u5b89\u88c5\u70b9 opt \u989d\u5916\u5b89\u88c5\u7684\u53ef\u9009\u5e94\u7528\u7a0b\u5e8f\u5305\u6240\u653e\u7f6e\u7684\u4f4d\u7f6e proc \u865a\u62df\u6587\u4ef6\u7cfb\u7edf\uff0c\u5b58\u653e\u5f53\u524d\u5185\u5b58\u7684\u6620\u5c04 root \u8d85\u7ea7\u7528\u6237\u76ee\u5f55 sbin \u5b58\u653e\u4e8c\u8fdb\u5236\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u53ea\u6709root\u624d\u80fd\u8bbf\u95ee tmp \u7528\u4e8e\u5b58\u653e\u5404\u79cd\u4e34\u65f6\u6587\u4ef6 usr \u7528\u4e8e\u5b58\u653e\u7cfb\u7edf\u5e94\u7528\u7a0b\u5e8f\uff0c\u6bd4\u8f83\u91cd\u8981\u7684\u76ee\u5f55/usr/local \u672c\u5730\u7ba1\u7406\u5458\u8f6f\u4ef6\u5b89\u88c5\u76ee\u5f55 var \u7528\u4e8e\u5b58\u653e\u8fd0\u884c\u65f6\u9700\u8981\u6539\u53d8\u6570\u636e\u7684\u6587\u4ef6 \u901a\u914d\u7b26 *\uff1a\u5339\u914d\u4efb\u4f55\u5b57\u7b26\u548c\u4efb\u4f55\u6570\u76ee\u7684\u5b57\u7b26 ?\uff1a\u5339\u914d\u5355\u4e00\u6570\u76ee\u7684\u4efb\u4f55\u5b57\u7b26 [ ]\uff1a\u5339\u914d[ ]\u4e4b\u5185\u7684\u4efb\u610f\u4e00\u4e2a\u5b57\u7b26 [! ]\uff1a\u5339\u914d\u9664\u4e86[! ]\u4e4b\u5916\u7684\u4efb\u610f\u4e00\u4e2a\u5b57\u7b26\uff0c!\u8868\u793a\u975e\u7684\u610f\u601d Linux\u57fa\u672c\u547d\u4ee4 pwd\uff1a\u67e5\u770b\u7528\u6237\u7684\u5f53\u524d\u76ee\u5f55 ls\uff1a\u663e\u793a\u6587\u4ef6\u6216\u76ee\u5f55\u4fe1\u606f mkdir\uff1a\u5f53\u524d\u76ee\u5f55\u4e0b\u521b\u5efa\u4e00\u4e2a\u7a7a\u76ee\u5f55 mv\uff1a\u79fb\u52a8\u6587\u4ef6\u6216\u76ee\u5f55\u3001\u6587\u4ef6\u6216\u76ee\u5f55\u6539\u540d rm\uff1a\u5220\u9664\u6587\u4ef6\u6216\u76ee\u5f55 ln\uff1a\u5efa\u7acb\u94fe\u63a5\u6587\u4ef6 find\uff1a\u67e5\u627e\u6587\u4ef6 file/stat\uff1a\u67e5\u770b\u6587\u4ef6\u7c7b\u578b\u6216\u6587\u4ef6\u5c5e\u6027\u4fe1\u606f cat\uff1a\u67e5\u770b\u6587\u672c\u6587\u4ef6\u5185\u5bb9 more\uff1a\u53ef\u4ee5\u5206\u9875\u770b less\uff1a\u4e0d\u4ec5\u53ef\u4ee5\u5206\u9875\uff0c\u8fd8\u53ef\u4ee5\u65b9\u4fbf\u5730\u641c\u7d22\uff0c\u56de\u7ffb\u7b49\u64cd\u4f5c tail -10\uff1a \u67e5\u770b\u6587\u4ef6\u7684\u5c3e\u90e8\u768410\u884c head -20\uff1a\u67e5\u770b\u6587\u4ef6\u7684\u5934\u90e820\u884c echo\uff1a\u628a\u5185\u5bb9\u91cd\u5b9a\u5411\u5230\u6307\u5b9a\u7684\u6587\u4ef6\u4e2d \uff0c\u6709\u5219\u6253\u5f00\uff0c\u65e0\u5219\u521b\u5efa cd \u66f4\u6539\u6587\u4ef6\u76ee\u5f55 \u5207\u6362\u5230\u4e3b\u76ee\u5f55 cd \u5207\u6362\u5230\u4e3b\u76ee\u5f55 cd ~ \u5207\u6362\u5230\u76ee\u5f55/tmp cd /tmp \u5207\u6362\u5230\u5f53\u524d\u76ee\u5f55\u7684dir\u76ee\u5f55 cd dir \u5207\u6362\u5230\u6839\u76ee\u5f55 cd / \u5207\u6362\u5230\u4e0a\u4e00\u7ea7\u76ee\u5f55 cd .. \u5207\u6362\u5230\u4e8c\u7ea7\u76ee\u5f55 cd ../.. \u5207\u6362\u5230\u4e3b\u76ee\u5f55\uff0c\u4f8b\u5982\u662froot\u7528\u6237\uff0c\u5219\u5207\u6362\u5230/root\u4e0b cd ~ \u538b\u7f29\u89e3\u538b\u547d\u4ee4 \u538b\u7f29 gzip filename bzip2 filename tar -czvf filename \u89e3\u538b gzip -d filename.gz bzip2 -d filename.bz2 tar -xzvf filename.tar.gz","title":"Linux"},{"location":"Linux/#linux","text":"bin \u5b58\u653e\u4e8c\u8fdb\u5236\u53ef\u6267\u884c\u6587\u4ef6(ls,cat,mkdir\u7b49) boot \u5b58\u653e\u7528\u4e8e\u7cfb\u7edf\u5f15\u5bfc\u65f6\u4f7f\u7528\u7684\u5404\u79cd\u6587\u4ef6 dev \u7528\u4e8e\u5b58\u653e\u8bbe\u5907\u6587\u4ef6 etc \u5b58\u653e\u7cfb\u7edf\u914d\u7f6e\u6587\u4ef6 home \u5b58\u653e\u6240\u6709\u7528\u6237\u6587\u4ef6\u7684\u6839\u76ee\u5f55 lib \u5b58\u653e\u8ddf\u6587\u4ef6\u7cfb\u7edf\u4e2d\u7684\u7a0b\u5e8f\u8fd0\u884c\u6240\u9700\u8981\u7684\u5171\u4eab\u5e93\u53ca\u5185\u6838\u6a21\u5757 mnt \u7cfb\u7edf\u7ba1\u7406\u5458\u5b89\u88c5\u4e34\u65f6\u6587\u4ef6\u7cfb\u7edf\u7684\u5b89\u88c5\u70b9 opt \u989d\u5916\u5b89\u88c5\u7684\u53ef\u9009\u5e94\u7528\u7a0b\u5e8f\u5305\u6240\u653e\u7f6e\u7684\u4f4d\u7f6e proc \u865a\u62df\u6587\u4ef6\u7cfb\u7edf\uff0c\u5b58\u653e\u5f53\u524d\u5185\u5b58\u7684\u6620\u5c04 root \u8d85\u7ea7\u7528\u6237\u76ee\u5f55 sbin \u5b58\u653e\u4e8c\u8fdb\u5236\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u53ea\u6709root\u624d\u80fd\u8bbf\u95ee tmp \u7528\u4e8e\u5b58\u653e\u5404\u79cd\u4e34\u65f6\u6587\u4ef6 usr \u7528\u4e8e\u5b58\u653e\u7cfb\u7edf\u5e94\u7528\u7a0b\u5e8f\uff0c\u6bd4\u8f83\u91cd\u8981\u7684\u76ee\u5f55/usr/local \u672c\u5730\u7ba1\u7406\u5458\u8f6f\u4ef6\u5b89\u88c5\u76ee\u5f55 var \u7528\u4e8e\u5b58\u653e\u8fd0\u884c\u65f6\u9700\u8981\u6539\u53d8\u6570\u636e\u7684\u6587\u4ef6","title":"Linux\u7cfb\u7edf\u76ee\u5f55\uff08\u5f85\u7ec6\u5316\uff09"},{"location":"Linux/#_1","text":"*\uff1a\u5339\u914d\u4efb\u4f55\u5b57\u7b26\u548c\u4efb\u4f55\u6570\u76ee\u7684\u5b57\u7b26 ?\uff1a\u5339\u914d\u5355\u4e00\u6570\u76ee\u7684\u4efb\u4f55\u5b57\u7b26 [ ]\uff1a\u5339\u914d[ ]\u4e4b\u5185\u7684\u4efb\u610f\u4e00\u4e2a\u5b57\u7b26 [! ]\uff1a\u5339\u914d\u9664\u4e86[! ]\u4e4b\u5916\u7684\u4efb\u610f\u4e00\u4e2a\u5b57\u7b26\uff0c!\u8868\u793a\u975e\u7684\u610f\u601d","title":"\u901a\u914d\u7b26"},{"location":"Linux/#linux_1","text":"pwd\uff1a\u67e5\u770b\u7528\u6237\u7684\u5f53\u524d\u76ee\u5f55 ls\uff1a\u663e\u793a\u6587\u4ef6\u6216\u76ee\u5f55\u4fe1\u606f mkdir\uff1a\u5f53\u524d\u76ee\u5f55\u4e0b\u521b\u5efa\u4e00\u4e2a\u7a7a\u76ee\u5f55 mv\uff1a\u79fb\u52a8\u6587\u4ef6\u6216\u76ee\u5f55\u3001\u6587\u4ef6\u6216\u76ee\u5f55\u6539\u540d rm\uff1a\u5220\u9664\u6587\u4ef6\u6216\u76ee\u5f55 ln\uff1a\u5efa\u7acb\u94fe\u63a5\u6587\u4ef6 find\uff1a\u67e5\u627e\u6587\u4ef6 file/stat\uff1a\u67e5\u770b\u6587\u4ef6\u7c7b\u578b\u6216\u6587\u4ef6\u5c5e\u6027\u4fe1\u606f cat\uff1a\u67e5\u770b\u6587\u672c\u6587\u4ef6\u5185\u5bb9 more\uff1a\u53ef\u4ee5\u5206\u9875\u770b less\uff1a\u4e0d\u4ec5\u53ef\u4ee5\u5206\u9875\uff0c\u8fd8\u53ef\u4ee5\u65b9\u4fbf\u5730\u641c\u7d22\uff0c\u56de\u7ffb\u7b49\u64cd\u4f5c tail -10\uff1a \u67e5\u770b\u6587\u4ef6\u7684\u5c3e\u90e8\u768410\u884c head -20\uff1a\u67e5\u770b\u6587\u4ef6\u7684\u5934\u90e820\u884c echo\uff1a\u628a\u5185\u5bb9\u91cd\u5b9a\u5411\u5230\u6307\u5b9a\u7684\u6587\u4ef6\u4e2d \uff0c\u6709\u5219\u6253\u5f00\uff0c\u65e0\u5219\u521b\u5efa","title":"Linux\u57fa\u672c\u547d\u4ee4"},{"location":"Linux/#cd","text":"\u5207\u6362\u5230\u4e3b\u76ee\u5f55 cd \u5207\u6362\u5230\u4e3b\u76ee\u5f55 cd ~ \u5207\u6362\u5230\u76ee\u5f55/tmp cd /tmp \u5207\u6362\u5230\u5f53\u524d\u76ee\u5f55\u7684dir\u76ee\u5f55 cd dir \u5207\u6362\u5230\u6839\u76ee\u5f55 cd / \u5207\u6362\u5230\u4e0a\u4e00\u7ea7\u76ee\u5f55 cd .. \u5207\u6362\u5230\u4e8c\u7ea7\u76ee\u5f55 cd ../.. \u5207\u6362\u5230\u4e3b\u76ee\u5f55\uff0c\u4f8b\u5982\u662froot\u7528\u6237\uff0c\u5219\u5207\u6362\u5230/root\u4e0b cd ~","title":"cd \u66f4\u6539\u6587\u4ef6\u76ee\u5f55"},{"location":"Linux/#_2","text":"\u538b\u7f29 gzip filename bzip2 filename tar -czvf filename \u89e3\u538b gzip -d filename.gz bzip2 -d filename.bz2 tar -xzvf filename.tar.gz","title":"\u538b\u7f29\u89e3\u538b\u547d\u4ee4"},{"location":"finetuneSAM/","text":"1\u5fae\u8c03\u7684\u76ee\u6807 Finetune SAM using the mask decoder Encoder-Decoder \u6a21\u578b\u662f\u4e00\u7c7b\u7b97\u6cd5\u7684\u7edf\u79f0\u3002Encoder-Decoder \u7b97\u662f\u4e00\u4e2a\u901a\u7528\u7684\u6846\u67b6. Encoder \u53c8\u79f0\u4f5c\u7f16\u7801\u5668\u3002\u5b83\u7684\u4f5c\u7528\u5c31\u662f\u300c\u5c06\u73b0\u5b9e\u95ee\u9898\u8f6c\u5316\u4e3a\u6570\u5b66\u95ee\u9898\u300d Decoder \u53c8\u79f0\u4f5c\u89e3\u7801\u5668\uff0c\u4ed6\u7684\u4f5c\u7528\u662f\u300c\u6c42\u89e3\u6570\u5b66\u95ee\u9898\uff0c\u5e76\u8f6c\u5316\u4e3a\u73b0\u5b9e\u4e16\u754c\u7684\u89e3\u51b3\u65b9\u6848\u300d SAM\u6a21\u578b\u7531\u4e09\u90e8\u5206\u7ec4\u6210\uff0c\u5206\u522b\u662fan image encoder, a prompt encoder and a mask decoder. 2why finetune? \u5bf9\u4e8e\u56fe\u7247\u6765\u8bf4\uff0c\u6211\u4eec\u795e\u7ecf\u7f51\u7edc\u524d\u51e0\u5c42\u5b66\u4e60\u5230\u7684\u90fd\u662f\u4f4e\u7ea7\u7684\u7279\u5f81\uff0c\u6bd4\u5982\uff0c\u70b9\u3001\u7ebf\u3001\u9762\uff0c\u8fd9\u4e9b\u4f4e\u7ea7\u7684\u7279\u5f81\u5bf9\u4e8e\u4efb\u4f55\u56fe\u7247\u6765\u8bf4\u90fd\u662f\u53ef\u4ee5\u62bd\u8c61\u51fa\u6765\u7684\uff0c\u6240\u4ee5\u6211\u4eec\u5c06\u4ed6\u4f5c\u4e3a\u901a\u7528\u6570\u636e\uff0c\u53ea\u5fae\u8c03\u8fd9\u4e9b\u4f4e\u7ea7\u7279\u5f81\u7ec4\u5408\u8d77\u6765\u7684\u9ad8\u7ea7\u7279\u5f81\u5373\u53ef\uff0c\u4f8b\u5982\uff0c\u8fd9\u4e9b\u70b9\u3001\u7ebf\u3001\u9762\uff0c\u7ec4\u6210\u7684\u662f\u5706\u8fd8\u662f\u692d\u5706\uff0c\u8fd8\u662f\u6b63\u65b9\u5f62\u3002 the information learn to recognise cats (edge detection, counting paws) will be useful for recognising dogs. 3\u9009\u62e9mask decoder image encoder\u7ed3\u6784\u590d\u6742\uff0c\u6709\u8bb8\u591a\u53c2\u6570\uff0c\u800c\u76f8\u5bf9\u6765\u8bf4mask decoder\u66f4\u8f7b\u91cf\uff0c\u66f4\u9ad8\u6548 \u4f46\u662f\u4e0d\u80fd\u76f4\u63a5\u8c03\u7528 SamPredictor.predict \u2014\u2014 \u56e0\u4e3aSamPredictor.predict_torch \u4e2d\u7684 torch.no_grad() \u963b\u6b62\u8ba1\u7b97\u68af\u5ea6 torch.no_grad()\u662f\u4e00\u4e2a\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u7528\u6765\u7981\u6b62\u68af\u5ea6\u7684\u8ba1\u7b97\uff0c\u901a\u5e38\u7528\u6765\u7f51\u7edc\u63a8\u65ad\u4e2d\uff0c\u5b83\u53ef\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5185\u5b58\u7684\u4f7f\u7528\u91cf\u3002 \u6df1\u5ea6\u5b66\u4e60\u4e2d\uff0c \u795e\u7ecf\u7f51\u7edc\u7684\u4e3b\u8981\u4efb\u52a1\u662f\u5728\u5b66\u4e60\u65f6\u627e\u5230\u6700\u4f18\u7684\u53c2\u6570\uff08\u6743\u91cd\u548c\u504f\u7f6e\uff09\uff0c\u8fd9\u4e2a\u6700\u4f18\u53c2\u6570\u4e5f\u5c31\u662f\u635f\u5931\u51fd\u6570\u6700\u5c0f\u65f6\u7684\u53c2\u6570\u3002\u4f46\u662f\uff0c\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u635f\u5931\u51fd\u6570\u6bd4\u8f83\u590d\u6742\uff0c\u53c2\u6570\u4e5f\u5f88\u591a\uff0c\u65e0\u6cd5\u786e\u5b9a\u5728\u54ea\u91cc\u53d6\u5f97\u6700\u5c0f\u503c\u3002\u6240\u4ee5\u901a\u8fc7\u68af\u5ea6\u6765\u5bfb\u627e\u6700\u5c0f\u503c\uff08\u6216\u8005\u5c3d\u53ef\u80fd\u5c0f\u7684\u503c\uff09\u7684\u65b9\u6cd5\u5c31\u662f\u68af\u5ea6\u6cd5\u3002 4\u5173\u4e8e\u81ea\u5b9a\u4e49\u6570\u636e\u96c6 \u5206\u5272\u7684\u56fe\u50cf \u771f\u503cground truth \u63d0\u793aprompt \u5728\u6559\u7a0b\u4e2d\uff0c\u4ed6\u4eec\u9009\u62e9\u4e86kaggle\u5e93\u4e2d\u7684\u90ae\u7968\u6570\u636e\u96c6\u2014\u2014 \u56e0\u4e3a\u5b83\u4e0a\u9762\u7684\u90ae\u6233\u662fSAM\u53ef\u80fd\u6ca1\u6709\u9884\u5148\u8bad\u7ec3\u8fc7\uff0c\u8868\u73b0\u96be\u4ee5\u5b8c\u7f8e \u6b64\u5916\u63d0\u4f9b\u4e86\u51c6\u786e\u7684ground truth masks\uff0c\u80fd\u591f\u8ba1\u7b97\u7cbe\u786e\u7684loss \u6570\u636e\u96c6\u8fd8\u63d0\u4f9b\u4e86\u5305\u542b\u771f\u503c\u63a9\u7801\u7684\u8fb9\u754c\u6846\uff0c\u53ef\u4ee5\u4f5c\u4e3aprompt 5\u73af\u5883\u642d\u5efa ! pip install kaggle &> /dev/null ! pip install torch torchvision &> /dev/null ! pip install opencv-python pycocotools matplotlib onnxruntime onnx &> /dev/null ! pip install git+https://github.com/facebookresearch/segment-anything.git &> /dev/null ! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth &> /dev/null \u8fd9\u4e9b\u547d\u4ee4\u662f\u5728Unix/Linux\u7cfb\u7edf\u4e0a\u8fd0\u884c\u7684\uff0c\u7528\u4e8e\u5b89\u88c5\u5404\u7c7b\u5305\uff0c\u5e76\u5c06\u8f93\u51fa\u91cd\u5b9a\u5411\u5230/dev/null\u3002\u5728\u5b89\u88c5\u5404\u7c7b\u5305\u65f6\u5c06\u6240\u6709\u8f93\u51fa\u4e22\u5f03\uff0c\u4e0d\u5728\u7ec8\u7aef\u4e0a\u663e\u793a\u6216\u8bb0\u5f55\u3002 Place your kaggle.json file into the files in the notebook workspace. More info here https://github.com/Kaggle/kaggle-api#api-credentials 6\u6570\u636e\u96c6\u51c6\u5907 ! mkdir ~/.kaggle ! mv kaggle.json ~/.kaggle/ ! chmod 600 ~/.kaggle/kaggle.json ! kaggle datasets download rtatman/stamp-verification-staver-dataset ! unzip stamp-verification-staver-dataset.zip &> /dev/null \u8bbe\u7f6eKaggle API\uff0c\u4e0b\u8f7d\u6570\u636e\u96c6 from pathlib import Path import numpy as np import matplotlib.pyplot as plt import cv2 # Exclude scans with zero or multiple bboxes (of the first 100) stamps_to_exclude = { 'stampDS-00008', 'stampDS-00010', 'stampDS-00015', 'stampDS-00021', 'stampDS-00027', 'stampDS-00031', 'stampDS-00039', 'stampDS-00041', 'stampDS-00049', 'stampDS-00053', 'stampDS-00059', 'stampDS-00069', 'stampDS-00073', 'stampDS-00080', 'stampDS-00090', 'stampDS-00098', 'stampDS-00100' }.union({ 'stampDS-00012', 'stampDS-00013', 'stampDS-00014', }) # Exclude 3 scans that aren't the type of scan we want to be fine tuning for \u524d100\u4e2a\u6570\u636e\u4e2d\uff0c\u53bb\u9664\u90ae\u6233\u6570\u636e\u4e3a\u7a7a\u7684\uff0c\u53bb\u9664\u4e0d\u60f3\u5fae\u8c03\u7684\u6570\u636e 7\u9884\u5904\u7406\u6570\u636e \u63d0\u53d6\u4f5c\u4e3aprompt\u7684\u8fb9\u754c\u6846\u5750\u6807 bbox_coords = {} for f in sorted(Path('ground-truth-maps/ground-truth-maps/').iterdir())[:100]: k = f.stem[:-3] if k not in stamps_to_exclude: im = cv2.imread(f.as_posix()) gray=cv2.cvtColor(im,cv2.COLOR_BGR2GRAY) contours, hierarchy = cv2.findContours(gray,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:] if len(contours) > 1: x,y,w,h = cv2.boundingRect(contours[0]) height, width, _ = im.shape bbox_coords[k] = np.array([x, y, x + w, y + h]) 1 \u904d\u5386\u6307\u5b9a\u76ee\u5f55\u4e2d\u7684\u524d100\u4e2a\u6587\u4ef6\u3002\u76ee\u5f55\u8def\u5f84\u662fground-truth-maps/ground-truth-maps/ 2 \u83b7\u53d6\u5f53\u524d\u6587\u4ef6\u7684\u6587\u4ef6\u540d\uff0c\u5e76\u53bb\u6389\u6587\u4ef6\u6269\u5c55\u540d\uff08\u540e\u7f00\uff09\uff0c\u4fdd\u5b58\u5728\u53d8\u91cfk\u4e2d 3 \u8bfb\u53d6\u5f53\u524d\u6587\u4ef6\u7684\u56fe\u50cf\u6570\u636e 4 \u5c06\u5f69\u8272\u56fe\u50cf\u8f6c\u6362\u4e3a\u7070\u5ea6\u56fe\u50cf\uff0c\u4fbf\u4e8e\u540e\u7eed\u7684\u5904\u7406 5 findContours\u51fd\u6570\u627e\u5230\u7070\u5ea6\u56fe\u50cf\u4e2d\u7684\u8f6e\u5ed3\u4fe1\u606f\uff0c\u5e76\u5c06\u8f6e\u5ed3\u548c\u5c42\u6b21\u7ed3\u6784\u5b58\u50a8\u5728contours\u548chierarchy\u53d8\u91cf\u4e2d cv2.RETR_LIST\u68c0\u6d4b\u7684\u8f6e\u5ed3\u4e0d\u5efa\u7acb\u7b49\u7ea7\u5173\u7cfb cv2.CHAIN_APPROX_SIMPLE\u538b\u7f29\u6c34\u5e73\u65b9\u5411\u3001\u5782\u76f4\u65b9\u5411\u3001\u5bf9\u89d2\u7ebf\u65b9\u5411\u7684\u5143\u7d20\uff0c\u53ea\u4fdd\u7559\u8be5\u65b9\u5411\u7684\u7ec8\u70b9\u5750\u6807\uff0c\u4f8b\u5982\u4e00\u4e2a\u77e9\u5f62\u8f6e\u5ed3\u53ea\u9700\u89814\u4e2a\u70b9\u6765\u4fdd\u5b58\u8f6e\u5ed3\u4fe1\u606f \uff1f cv2.findContours()\u51fd\u6570\u63a5\u53d7\u7684\u53c2\u6570\u4e3a\u4e8c\u503c\u56fe\uff0c\u5373\u9ed1\u767d\u7684\uff08\u4e0d\u662f\u7070\u5ea6\u56fe\uff09\uff0c\u6240\u4ee5\u8bfb\u53d6\u7684\u56fe\u50cf\u8981\u5148\u8f6c\u6210\u7070\u5ea6\u7684\uff0c\u518d\u8f6c\u6210\u4e8c\u503c\u56fe\u3002 6 \u8ba1\u7b97\u7b2c\u4e00\u4e2a\u8f6e\u5ed3\u7684\u8fb9\u754c\u77e9\u5f62 7 \u83b7\u53d6\u56fe\u50cf\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6 8 \u5c06\u8fb9\u754c\u6846\u7684\u5750\u6807\u4fe1\u606f\u5b58\u50a8\u5728\u5b57\u5178bbox_coords\u4e2d \u904d\u5386\u4e00\u7ec4\u56fe\u50cf\u6587\u4ef6\uff0c\u5bf9\u6bcf\u4e2a\u56fe\u50cf\u63d0\u53d6\u8fb9\u754c\u6846\u5750\u6807\u4fe1\u606f\uff0c\u5e76\u5c06\u5176\u5b58\u50a8\u5728\u5b57\u5178\u4e2d\u3002 \u83b7\u5f97\u771f\u503c\u63a9\u7801 ground_truth_masks = {} for k in bbox_coords.keys(): gt_grayscale = cv2.imread(f'ground-truth-pixel/ground-truth-pixel/{k}-px.png', cv2.IMREAD_GRAYSCALE) ground_truth_masks[k] = (gt_grayscale == 0) 1 \u4f7f\u7528imread\u4ee5\u7070\u5ea6\u56fe\u50cf\u7684\u5f62\u5f0f\u8bfb\u53d6\u6307\u5b9a\u8def\u5f84\u4e0b\u7684\u771f\u503c\u63a9\u7801\u56fe\u50cf 2 \u5c06\u5f53\u524d\u8fb9\u754c\u6846\u7684\u6807\u8bc6\u4f5c\u4e3a\u952e\uff0c\u5c06\u901a\u8fc7\u6bd4\u8f83\u771f\u503c\u63a9\u7801\u56fe\u50cf\u4e2d\u7684\u50cf\u7d20\u503c\u662f\u5426\u4e3a0\u6240\u5f97\u5230\u7684\u5e03\u5c14\u6570\u7ec4\u4f5c\u4e3a\u503c\uff0c\u5b58\u50a8\u5728ground_truth_masks\u5b57\u5178\u4e2d \u6839\u636e\u8fb9\u754c\u6846\u7684\u6807\u8bc6\uff0c\u8bfb\u53d6\u5bf9\u5e94\u7684\u771f\u503c\u63a9\u7801\u56fe\u50cf\uff0c\u5e76\u5c06\u63a9\u7801\u8f6c\u6362\u4e3a\u5e03\u5c14\u6570\u7ec4\u8868\u793a 8\u67e5\u770bimage prompt ground truth def show_mask(mask, ax, random_color=False): if random_color: color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0) else: color = np.array([30/255, 144/255, 255/255, 0.6]) h, w = mask.shape[-2:] mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1) ax.imshow(mask_image) def show_box(box, ax): x0, y0 = box[0], box[1] w, h = box[2] - box[0], box[3] - box[1] ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2)) \u5b9a\u4e49\u4e86\u4e24\u4e2a\u51fd\u6570 show_mask() \u548c show_box()\uff0c\u7528\u4e8e\u5728Matplotlib\u4e0a\u663e\u793a\u63a9\u7801\u548c\u8fb9\u754c\u6846 name = 'stampDS-00004' image = cv2.imread(f'scans/scans/{name}.png') plt.figure(figsize=(10,10)) plt.imshow(image) show_box(bbox_coords[name], plt.gca()) show_mask(ground_truth_masks[name], plt.gca()) plt.axis('off') plt.show() ground truth mask\u975e\u5e38\u51c6\u786e\uff0c\u8fd9\u6709\u5229\u4e8e\u8ba1\u7b97\u51c6\u786e\u7684loss\u3002\u8fb9\u754c\u6846\u5c06\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u63d0\u793a\u3002 9\u5fae\u8c03\u51c6\u5907 model_type = 'vit_b' checkpoint = 'sam_vit_b_01ec64.pth' device = 'cuda:0' from segment_anything import SamPredictor, sam_model_registry sam_model = sam_model_registry[model_type](checkpoint=checkpoint) sam_model.to(device) sam_model.train(); \u5c06\u8f93\u5165\u56fe\u50cf\u8f6c\u6362\u4e3aSAM\u5185\u90e8\u51fd\u6570\u671f\u671b\u7684\u683c\u5f0f(\u9884\u5904\u7406\u56fe\u50cf) from collections import defaultdict import torch from segment_anything.utils.transforms import ResizeLongestSide transformed_data = defaultdict(dict) for k in bbox_coords.keys(): image = cv2.imread(f'scans/scans/{k}.png') image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) transform = ResizeLongestSide(sam_model.image_encoder.img_size) input_image = transform.apply_image(image) input_image_torch = torch.as_tensor(input_image, device=device) transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :] input_image = sam_model.preprocess(transformed_image) original_image_size = image.shape[:2] input_size = tuple(transformed_image.shape[-2:]) transformed_data[k]['image'] = input_image transformed_data[k]['input_size'] = input_size transformed_data[k]['original_image_size'] = original_image_size 1 \u8bfb\u53d6\u56fe\u50cf\u6587\u4ef6 f'scans/scans/{k}.png' \u5e76\u5b58\u50a8\u5728\u53d8\u91cf image 2 \u4f7f\u7528 cv2.cvtColor() \u5c06\u56fe\u50cf\u4ece BGR \u683c\u5f0f\u8f6c\u6362\u4e3a RGB \u683c\u5f0f 3 ResizeLongestSide\u5c06\u56fe\u50cf\u5927\u5c0f\u8c03\u6574\u4e3a sam_model.image_encoder.img_size 4 \u4f7f\u7528 torch.as_tensor() \u5c06 input_image \u8f6c\u6362\u4e3a Torch \u5f20\u91cf 5 \u4f7f\u7528 permute() \u8c03\u6574\u5f20\u91cf\u7684\u7ef4\u5ea6\u987a\u5e8f\uff0c\u5c06\u901a\u9053\u7ef4\u5ea6\u653e\u5728\u7b2c\u4e00\u7ef4\uff0c\u5e76\u4f7f\u7528 contiguous() \u4f7f\u6570\u636e\u5728\u5185\u5b58\u4e2d\u8fde\u7eed\u5b58\u50a8\uff0c\u5728\u7b2c\u4e00\u7ef4\u4e0a\u6dfb\u52a0\u4e00\u4e2a\u7ef4\u5ea6\uff0c\u5f97\u5230\u5f62\u72b6\u4e3a [1, C, H, W] \u7684\u5f20\u91cf transformed_image 6 \u4f7f\u7528 sam_model.preprocess() \u5bf9 transformed_image \u8fdb\u884c\u9884\u5904\u7406\uff0c\u5f97\u5230\u6a21\u578b\u7684\u8f93\u5165 7 \u83b7\u53d6\u539f\u59cb\u56fe\u50cf\u7684\u5c3a\u5bf8\u548c\u53d8\u6362\u540e\u56fe\u50cf\u7684\u5c3a\u5bf8\uff0c\u5e76\u5b58\u50a8\u5728 original_image_size \u548c input_size \u4e2d 8 \u5c06\u8f6c\u6362\u540e\u7684\u6570\u636e\u5b58\u50a8\u5728 transformed_data[k] \u5b57\u5178\u4e2d\uff0c\u5305\u62ec\u8f93\u5165\u56fe\u50cf input_image\u3001\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8 input_size \u548c\u539f\u59cb\u56fe\u50cf\u5c3a\u5bf8 original_image_size \u5c06\u56fe\u50cf\u6570\u636e\u8fdb\u884c\u8f6c\u6362\u548c\u9884\u5904\u7406\uff0c\u5e76\u5c06\u8f6c\u6362\u540e\u7684\u6570\u636e\u5b58\u50a8\u5728 transformed_data \u5b57\u5178\u4e2d 10\u8bbe\u7f6e\u4f18\u5316\u5668 lr = 1e-4 wd = 0 optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=wd) loss_fn = torch.nn.MSELoss() # loss_fn = torch.nn.BCELoss() keys = list(bbox_coords.keys()) \u5b66\u4e60\u7387 \uff08\u6b65\u957f\uff09 \u6743\u91cd\u8870\u51cf \u521b\u5efa\u4e00\u4e2a Adam \u4f18\u5316\u5668\u5bf9\u8c61\u3002\u8be5\u4f18\u5316\u5668\u7528\u4e8e\u66f4\u65b0 sam_model.mask_decoder \u4e2d\u7684\u53c2\u6570 \u635f\u5931\u51fd\u6570 \u521b\u5efa\u4e00\u4e2a\u5747\u65b9\u8bef\u5dee\u635f\u5931\u51fd\u6570\u5bf9\u8c61 11\u8fd0\u884c\u5fae\u8c03 \u8981\u505a\u7684\u6539\u8fdb\u5305\u62ec\u6279\u5904\u7406\u548c\u79fb\u52a8\u56fe\u50cf\u7684\u8ba1\u7b97\uff0c\u5e76\u5728\u5faa\u73af\u4e4b\u5916\u63d0\u793a\u5d4c\u5165\uff0c\u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u8c03\u6574\u6a21\u578b\u7684\u8fd9\u4e9b\u90e8\u5206\uff0c\u8fd9\u5c06\u52a0\u5feb\u8bad\u7ec3\u901f\u5ea6\uff0c\u56e0\u4e3a\u6211\u4eec\u4e0d\u5e94\u8be5\u5728\u6bcf\u4e2a\u65f6\u671f\u91cd\u65b0\u8ba1\u7b97\u5d4c\u5165\u3002 \u6709\u65f6\u4f18\u5316\u5668\u4f1a\u8ff7\u5931\u5728\u53c2\u6570\u7a7a\u95f4\u4e2d\uff0c\u635f\u5931\u51fd\u6570\u4f1a\u5d29\u6e83\u3002 \u4ece\u5934\u5f00\u59cb\uff08\u5305\u62ec\u8fd0\u884c\u201c\u51c6\u5907\u5fae\u8c03\u201d\u4e0b\u7684\u6240\u6709\u5355\u5143\u683c\u4ee5\u4fbf\u518d\u6b21\u4f7f\u7528\u9ed8\u8ba4\u6743\u91cd\u5f00\u59cb\uff09\u5c06\u89e3\u51b3\u6b64\u95ee\u9898\u3002 from statistics import mean from tqdm import tqdm from torch.nn.functional import threshold, normalize num_epochs = 100 losses = [] for epoch in range(num_epochs): epoch_losses = [] # Just train on the first 20 examples for k in keys[:20]: input_image = transformed_data[k]['image'].to(device) input_size = transformed_data[k]['input_size'] original_image_size = transformed_data[k]['original_image_size'] # No grad here as we don't want to optimise the encoders with torch.no_grad(): image_embedding = sam_model.image_encoder(input_image) prompt_box = bbox_coords[k] box = transform.apply_boxes(prompt_box, original_image_size) box_torch = torch.as_tensor(box, dtype=torch.float, device=device) box_torch = box_torch[None, :] sparse_embeddings, dense_embeddings = sam_model.prompt_encoder( points=None, boxes=box_torch, masks=None, ) low_res_masks, iou_predictions = sam_model.mask_decoder( image_embeddings=image_embedding, image_pe=sam_model.prompt_encoder.get_dense_pe(), sparse_prompt_embeddings=sparse_embeddings, dense_prompt_embeddings=dense_embeddings, multimask_output=False, ) upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device) binary_mask = normalize(threshold(upscaled_masks, 0.0, 0)) gt_mask_resized = torch.from_numpy(np.resize(ground_truth_masks[k], (1, 1, ground_truth_masks[k].shape[0], ground_truth_masks[k].shape[1]))).to(device) gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32) loss = loss_fn(binary_mask, gt_binary_mask) optimizer.zero_grad() loss.backward() optimizer.step() epoch_losses.append(loss.item()) losses.append(epoch_losses) print(f'EPOCH: {epoch}') print(f'Mean loss: {mean(epoch_losses)}') 1 num_epochs = 100 \u8bbe\u7f6e\u4e86\u603b\u7684\u8bad\u7ec3\u8f6e\u6570 2 \u521b\u5efa\u4e00\u4e2a\u7a7a\u5217\u8868 epoch_losses\uff0c\u7528\u4e8e\u5b58\u50a8\u5f53\u524d\u8f6e\u6b21\u4e2d\u6bcf\u4e2a\u6837\u672c\u7684\u635f\u5931\u503c 3 \u4ece transformed_data \u5b57\u5178\u4e2d\u83b7\u53d6\u8f93\u5165\u56fe\u50cf input_image\u3001\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8 input_size \u548c\u539f\u59cb\u56fe\u50cf\u5c3a\u5bf8 original_image_size 4 \u4f7f\u7528 sam_model.image_encoder \u5bf9\u8f93\u5165\u56fe\u50cf\u8fdb\u884c\u7f16\u7801\uff0c\u5f97\u5230\u56fe\u50cf\u5d4c\u5165 image_embedding 5 \u4f7f\u7528 transform.apply_boxes() \u5c06\u8fb9\u754c\u6846\u4ece\u53d8\u6362\u540e\u7684\u56fe\u50cf\u5c3a\u5bf8\u8fd8\u539f\u5230\u539f\u59cb\u56fe\u50cf\u5c3a\u5bf8\uff0c\u5c06\u8fb9\u754c\u6846\u8f6c\u6362\u4e3a Torch \u5f20\u91cf 6 \u4f7f\u7528 sam_model.prompt_encoder \u5bf9\u7a00\u758f\u5d4c\u5165\u548c\u5bc6\u96c6\u5d4c\u5165\u8fdb\u884c\u7f16\u7801\uff0c\u5f97\u5230\u76f8\u5e94\u7684\u5d4c\u5165\u5f20\u91cf\u3002 7 \u4f7f\u7528 sam_model.mask_decoder \u5bf9\u56fe\u50cf\u5d4c\u5165\u548c\u5d4c\u5165\u5f20\u91cf\u8fdb\u884c\u89e3\u7801\uff0c\u5f97\u5230\u4f4e\u5206\u8fa8\u7387\u7684\u63a9\u6a21 low_res_masks \u548c IOU \u9884\u6d4b\u7ed3\u679c iou_predictions 8 \u5c06\u4f4e\u5206\u8fa8\u7387\u7684\u63a9\u6a21\u8c03\u6574\u5c3a\u5bf8\uff0c\u5e76\u5c06\u5176\u4e8c\u503c\u5316\u4e3a\u4e8c\u8fdb\u5236\u63a9\u6a21 9 \u4ece ground_truth_masks \u5b57\u5178\u4e2d\u83b7\u53d6\u76ee\u6807\u4e8c\u8fdb\u5236\u63a9\u6a21 gt_binary_mask 10 \u8ba1\u7b97loss\uff0c\u901a\u8fc7\u5c06\u4e8c\u8fdb\u5236\u63a9\u6a21\u548c\u76ee\u6807\u63a9\u6a21\u8f93\u5165\u635f\u5931\u51fd\u6570\u8fdb\u884c\u8ba1\u7b97 11 \u6e05\u9664\u4f18\u5316\u5668\u7684\u68af\u5ea6\u4fe1\u606f\u3002 12 \u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u8ba1\u7b97\u68af\u5ea6\u3002 13 \u6267\u884c\u4f18\u5316\u5668\u7684\u53c2\u6570\u66f4\u65b0\u3002 14 \u5c06\u635f\u5931\u503c\u6dfb\u52a0\u5230 epoch_losses \u5217\u8868\u4e2d mean_losses = [mean(x) for x in losses] mean_losses plt.plot(list(range(len(mean_losses))), mean_losses) plt.title('Mean epoch loss') plt.xlabel('Epoch Number') plt.ylabel('Loss') plt.show() \u4f7f\u7528\u524d\u9762\u8ba1\u7b97\u5f97\u5230\u7684 losses \u5217\u8868\u8ba1\u7b97\u4e86\u6bcf\u4e2a\u8bad\u7ec3\u8f6e\u6b21\u7684\u5e73\u5747\u635f\u5931\u503c\uff0c\u5e76\u8fdb\u884c\u4e86\u53ef\u89c6\u5316 12\u6bd4\u8f83\u5fae\u8c03\u524d\u540e\u7684\u6a21\u578b sam_model_orig = sam_model_registry[model_type](checkpoint=checkpoint) sam_model_orig.to(device); from segment_anything import sam_model_registry, SamPredictor predictor_tuned = SamPredictor(sam_model) predictor_original = SamPredictor(sam_model_orig) k = keys[21] image = cv2.imread(f'scans/scans/{k}.png') image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) predictor_tuned.set_image(image) predictor_original.set_image(image) input_bbox = np.array(bbox_coords[k]) masks_tuned, _, _ = predictor_tuned.predict( point_coords=None, box=input_bbox, multimask_output=False, ) masks_orig, _, _ = predictor_original.predict( point_coords=None, box=input_bbox, multimask_output=False, ) 1 \u4f7f\u7528\u8c03\u4f18\u540e\u7684\u6a21\u578b\u7684\u9884\u6d4b\u5668\u5bf9\u8c61\u5bf9\u5f85\u9884\u6d4b\u56fe\u50cf\u8fdb\u884c\u9884\u6d4b\u3002\u9884\u6d4b\u7684\u7ed3\u679c\u5b58\u50a8\u5728 masks_tuned 2 \u4f7f\u7528\u539f\u59cb\u6a21\u578b\u7684\u9884\u6d4b\u5668\u5bf9\u8c61\u5bf9\u5f85\u9884\u6d4b\u56fe\u50cf\u8fdb\u884c\u9884\u6d4b\u3002\u9884\u6d4b\u7684\u7ed3\u679c\u5b58\u50a8\u5728 masks_orig %matplotlib inline _, axs = plt.subplots(1, 2, figsize=(25, 25)) axs[0].imshow(image) show_mask(masks_tuned, axs[0]) show_box(input_bbox, axs[0]) axs[0].set_title('Mask with Tuned Model', fontsize=26) axs[0].axis('off') axs[1].imshow(image) show_mask(masks_orig, axs[1]) show_box(input_bbox, axs[1]) axs[1].set_title('Mask with Untuned Model', fontsize=26) axs[1].axis('off') plt.show() penCV-Python\u63a5\u53e3\u4e2d\u4f7f\u7528cv2.findContours()\u51fd\u6570\u6765\u67e5\u627e\u68c0\u6d4b\u7269\u4f53\u7684\u8f6e\u5ed3\u3002 contours, hierarchy = cv2.findContours(image,mode,method) image\uff1a\u8f93\u5165\u56fe\u50cf mode\uff1a\u8f6e\u5ed3\u7684\u6a21\u5f0f\u3002cv2.RETR_EXTERNAL\u53ea\u68c0\u6d4b\u5916\u8f6e\u5ed3\uff1bcv2.RETR_LIST\u68c0\u6d4b\u7684\u8f6e\u5ed3\u4e0d\u5efa\u7acb\u7b49\u7ea7\u5173\u7cfb\uff1bcv2.RETR_CCOMP\u5efa\u7acb\u4e24\u4e2a\u7b49\u7ea7\u7684\u8f6e\u5ed3\uff0c\u4e0a\u4e00\u5c42\u4e3a\u5916\u8fb9\u754c\uff0c\u5185\u5c42\u4e3a\u5185\u5b54\u7684\u8fb9\u754c\u3002\u5982\u679c\u5185\u5b54\u5185\u8fd8\u6709\u8fde\u901a\u7269\u4f53\uff0c\u5219\u8fd9\u4e2a\u7269\u4f53\u7684\u8fb9\u754c\u4e5f\u5728\u9876\u5c42\uff1bcv2.RETR_TREE\u5efa\u7acb\u4e00\u4e2a\u7b49\u7ea7\u6811\u7ed3\u6784\u7684\u8f6e\u5ed3\u3002 method\uff1a\u8f6e\u5ed3\u7684\u8fd1\u4f3c\u65b9\u6cd5\u3002cv2.CHAIN_APPROX_NOME\u5b58\u50a8\u6240\u6709\u7684\u8f6e\u5ed3\u70b9\uff0c\u76f8\u90bb\u7684\u4e24\u4e2a\u70b9\u7684\u50cf\u7d20\u4f4d\u7f6e\u5dee\u4e0d\u8d85\u8fc71\uff1bcv2.CHAIN_APPROX_SIMPLE\u538b\u7f29\u6c34\u5e73\u65b9\u5411\u3001\u5782\u76f4\u65b9\u5411\u3001\u5bf9\u89d2\u7ebf\u65b9\u5411\u7684\u5143\u7d20\uff0c\u53ea\u4fdd\u7559\u8be5\u65b9\u5411\u7684\u7ec8\u70b9\u5750\u6807\uff0c\u4f8b\u5982\u4e00\u4e2a\u77e9\u5f62\u8f6e\u5ed3\u53ea\u9700\u89814\u4e2a\u70b9\u6765\u4fdd\u5b58\u8f6e\u5ed3\u4fe1\u606f\uff1bcv2.CHAIN_APPROX_TC89_L1\uff0ccv2.CV_CHAIN_APPROX_TC89_KCOS contours\uff1a\u8fd4\u56de\u7684\u8f6e\u5ed3 hierarchy\uff1a\u6bcf\u6761\u8f6e\u5ed3\u5bf9\u5e94\u7684\u5c5e\u6027 grad\u5728\u53cd\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u662f\u7d2f\u52a0\u7684(accumulated)\uff0c\u8fd9\u610f\u5473\u7740\u6bcf\u4e00\u6b21\u8fd0\ufa08\u53cd\u5411\u4f20\u64ad\uff0c\u68af\u5ea6\u90fd\u4f1a\u7d2f\u52a0\u4e4b\u524d\u7684\u68af\u5ea6\uff0c\u6240\u4ee5\u4e00\u822c\u5728\u53cd\u5411\u4f20\u64ad\u4e4b\u524d\u9700\u628a\u68af\u5ea6\u6e05\u96f6\u3002","title":"FinetuneSAM"},{"location":"finetuneSAM/#_1","text":"","title":""},{"location":"finetuneSAM/#1","text":"Finetune SAM using the mask decoder Encoder-Decoder \u6a21\u578b\u662f\u4e00\u7c7b\u7b97\u6cd5\u7684\u7edf\u79f0\u3002Encoder-Decoder \u7b97\u662f\u4e00\u4e2a\u901a\u7528\u7684\u6846\u67b6. Encoder \u53c8\u79f0\u4f5c\u7f16\u7801\u5668\u3002\u5b83\u7684\u4f5c\u7528\u5c31\u662f\u300c\u5c06\u73b0\u5b9e\u95ee\u9898\u8f6c\u5316\u4e3a\u6570\u5b66\u95ee\u9898\u300d Decoder \u53c8\u79f0\u4f5c\u89e3\u7801\u5668\uff0c\u4ed6\u7684\u4f5c\u7528\u662f\u300c\u6c42\u89e3\u6570\u5b66\u95ee\u9898\uff0c\u5e76\u8f6c\u5316\u4e3a\u73b0\u5b9e\u4e16\u754c\u7684\u89e3\u51b3\u65b9\u6848\u300d SAM\u6a21\u578b\u7531\u4e09\u90e8\u5206\u7ec4\u6210\uff0c\u5206\u522b\u662fan image encoder, a prompt encoder and a mask decoder.","title":"1\u5fae\u8c03\u7684\u76ee\u6807"},{"location":"finetuneSAM/#2why-finetune","text":"\u5bf9\u4e8e\u56fe\u7247\u6765\u8bf4\uff0c\u6211\u4eec\u795e\u7ecf\u7f51\u7edc\u524d\u51e0\u5c42\u5b66\u4e60\u5230\u7684\u90fd\u662f\u4f4e\u7ea7\u7684\u7279\u5f81\uff0c\u6bd4\u5982\uff0c\u70b9\u3001\u7ebf\u3001\u9762\uff0c\u8fd9\u4e9b\u4f4e\u7ea7\u7684\u7279\u5f81\u5bf9\u4e8e\u4efb\u4f55\u56fe\u7247\u6765\u8bf4\u90fd\u662f\u53ef\u4ee5\u62bd\u8c61\u51fa\u6765\u7684\uff0c\u6240\u4ee5\u6211\u4eec\u5c06\u4ed6\u4f5c\u4e3a\u901a\u7528\u6570\u636e\uff0c\u53ea\u5fae\u8c03\u8fd9\u4e9b\u4f4e\u7ea7\u7279\u5f81\u7ec4\u5408\u8d77\u6765\u7684\u9ad8\u7ea7\u7279\u5f81\u5373\u53ef\uff0c\u4f8b\u5982\uff0c\u8fd9\u4e9b\u70b9\u3001\u7ebf\u3001\u9762\uff0c\u7ec4\u6210\u7684\u662f\u5706\u8fd8\u662f\u692d\u5706\uff0c\u8fd8\u662f\u6b63\u65b9\u5f62\u3002 the information learn to recognise cats (edge detection, counting paws) will be useful for recognising dogs.","title":"2why finetune?"},{"location":"finetuneSAM/#3mask-decoder","text":"image encoder\u7ed3\u6784\u590d\u6742\uff0c\u6709\u8bb8\u591a\u53c2\u6570\uff0c\u800c\u76f8\u5bf9\u6765\u8bf4mask decoder\u66f4\u8f7b\u91cf\uff0c\u66f4\u9ad8\u6548 \u4f46\u662f\u4e0d\u80fd\u76f4\u63a5\u8c03\u7528 SamPredictor.predict \u2014\u2014 \u56e0\u4e3aSamPredictor.predict_torch \u4e2d\u7684 torch.no_grad() \u963b\u6b62\u8ba1\u7b97\u68af\u5ea6 torch.no_grad()\u662f\u4e00\u4e2a\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u7528\u6765\u7981\u6b62\u68af\u5ea6\u7684\u8ba1\u7b97\uff0c\u901a\u5e38\u7528\u6765\u7f51\u7edc\u63a8\u65ad\u4e2d\uff0c\u5b83\u53ef\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5185\u5b58\u7684\u4f7f\u7528\u91cf\u3002 \u6df1\u5ea6\u5b66\u4e60\u4e2d\uff0c \u795e\u7ecf\u7f51\u7edc\u7684\u4e3b\u8981\u4efb\u52a1\u662f\u5728\u5b66\u4e60\u65f6\u627e\u5230\u6700\u4f18\u7684\u53c2\u6570\uff08\u6743\u91cd\u548c\u504f\u7f6e\uff09\uff0c\u8fd9\u4e2a\u6700\u4f18\u53c2\u6570\u4e5f\u5c31\u662f\u635f\u5931\u51fd\u6570\u6700\u5c0f\u65f6\u7684\u53c2\u6570\u3002\u4f46\u662f\uff0c\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u635f\u5931\u51fd\u6570\u6bd4\u8f83\u590d\u6742\uff0c\u53c2\u6570\u4e5f\u5f88\u591a\uff0c\u65e0\u6cd5\u786e\u5b9a\u5728\u54ea\u91cc\u53d6\u5f97\u6700\u5c0f\u503c\u3002\u6240\u4ee5\u901a\u8fc7\u68af\u5ea6\u6765\u5bfb\u627e\u6700\u5c0f\u503c\uff08\u6216\u8005\u5c3d\u53ef\u80fd\u5c0f\u7684\u503c\uff09\u7684\u65b9\u6cd5\u5c31\u662f\u68af\u5ea6\u6cd5\u3002","title":"3\u9009\u62e9mask decoder"},{"location":"finetuneSAM/#4","text":"\u5206\u5272\u7684\u56fe\u50cf \u771f\u503cground truth \u63d0\u793aprompt \u5728\u6559\u7a0b\u4e2d\uff0c\u4ed6\u4eec\u9009\u62e9\u4e86kaggle\u5e93\u4e2d\u7684\u90ae\u7968\u6570\u636e\u96c6\u2014\u2014 \u56e0\u4e3a\u5b83\u4e0a\u9762\u7684\u90ae\u6233\u662fSAM\u53ef\u80fd\u6ca1\u6709\u9884\u5148\u8bad\u7ec3\u8fc7\uff0c\u8868\u73b0\u96be\u4ee5\u5b8c\u7f8e \u6b64\u5916\u63d0\u4f9b\u4e86\u51c6\u786e\u7684ground truth masks\uff0c\u80fd\u591f\u8ba1\u7b97\u7cbe\u786e\u7684loss \u6570\u636e\u96c6\u8fd8\u63d0\u4f9b\u4e86\u5305\u542b\u771f\u503c\u63a9\u7801\u7684\u8fb9\u754c\u6846\uff0c\u53ef\u4ee5\u4f5c\u4e3aprompt","title":"4\u5173\u4e8e\u81ea\u5b9a\u4e49\u6570\u636e\u96c6"},{"location":"finetuneSAM/#5","text":"! pip install kaggle &> /dev/null ! pip install torch torchvision &> /dev/null ! pip install opencv-python pycocotools matplotlib onnxruntime onnx &> /dev/null ! pip install git+https://github.com/facebookresearch/segment-anything.git &> /dev/null ! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth &> /dev/null \u8fd9\u4e9b\u547d\u4ee4\u662f\u5728Unix/Linux\u7cfb\u7edf\u4e0a\u8fd0\u884c\u7684\uff0c\u7528\u4e8e\u5b89\u88c5\u5404\u7c7b\u5305\uff0c\u5e76\u5c06\u8f93\u51fa\u91cd\u5b9a\u5411\u5230/dev/null\u3002\u5728\u5b89\u88c5\u5404\u7c7b\u5305\u65f6\u5c06\u6240\u6709\u8f93\u51fa\u4e22\u5f03\uff0c\u4e0d\u5728\u7ec8\u7aef\u4e0a\u663e\u793a\u6216\u8bb0\u5f55\u3002 Place your kaggle.json file into the files in the notebook workspace. More info here https://github.com/Kaggle/kaggle-api#api-credentials","title":"5\u73af\u5883\u642d\u5efa"},{"location":"finetuneSAM/#6","text":"! mkdir ~/.kaggle ! mv kaggle.json ~/.kaggle/ ! chmod 600 ~/.kaggle/kaggle.json ! kaggle datasets download rtatman/stamp-verification-staver-dataset ! unzip stamp-verification-staver-dataset.zip &> /dev/null \u8bbe\u7f6eKaggle API\uff0c\u4e0b\u8f7d\u6570\u636e\u96c6 from pathlib import Path import numpy as np import matplotlib.pyplot as plt import cv2 # Exclude scans with zero or multiple bboxes (of the first 100) stamps_to_exclude = { 'stampDS-00008', 'stampDS-00010', 'stampDS-00015', 'stampDS-00021', 'stampDS-00027', 'stampDS-00031', 'stampDS-00039', 'stampDS-00041', 'stampDS-00049', 'stampDS-00053', 'stampDS-00059', 'stampDS-00069', 'stampDS-00073', 'stampDS-00080', 'stampDS-00090', 'stampDS-00098', 'stampDS-00100' }.union({ 'stampDS-00012', 'stampDS-00013', 'stampDS-00014', }) # Exclude 3 scans that aren't the type of scan we want to be fine tuning for \u524d100\u4e2a\u6570\u636e\u4e2d\uff0c\u53bb\u9664\u90ae\u6233\u6570\u636e\u4e3a\u7a7a\u7684\uff0c\u53bb\u9664\u4e0d\u60f3\u5fae\u8c03\u7684\u6570\u636e","title":"6\u6570\u636e\u96c6\u51c6\u5907"},{"location":"finetuneSAM/#7","text":"\u63d0\u53d6\u4f5c\u4e3aprompt\u7684\u8fb9\u754c\u6846\u5750\u6807 bbox_coords = {} for f in sorted(Path('ground-truth-maps/ground-truth-maps/').iterdir())[:100]: k = f.stem[:-3] if k not in stamps_to_exclude: im = cv2.imread(f.as_posix()) gray=cv2.cvtColor(im,cv2.COLOR_BGR2GRAY) contours, hierarchy = cv2.findContours(gray,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:] if len(contours) > 1: x,y,w,h = cv2.boundingRect(contours[0]) height, width, _ = im.shape bbox_coords[k] = np.array([x, y, x + w, y + h]) 1 \u904d\u5386\u6307\u5b9a\u76ee\u5f55\u4e2d\u7684\u524d100\u4e2a\u6587\u4ef6\u3002\u76ee\u5f55\u8def\u5f84\u662fground-truth-maps/ground-truth-maps/ 2 \u83b7\u53d6\u5f53\u524d\u6587\u4ef6\u7684\u6587\u4ef6\u540d\uff0c\u5e76\u53bb\u6389\u6587\u4ef6\u6269\u5c55\u540d\uff08\u540e\u7f00\uff09\uff0c\u4fdd\u5b58\u5728\u53d8\u91cfk\u4e2d 3 \u8bfb\u53d6\u5f53\u524d\u6587\u4ef6\u7684\u56fe\u50cf\u6570\u636e 4 \u5c06\u5f69\u8272\u56fe\u50cf\u8f6c\u6362\u4e3a\u7070\u5ea6\u56fe\u50cf\uff0c\u4fbf\u4e8e\u540e\u7eed\u7684\u5904\u7406 5 findContours\u51fd\u6570\u627e\u5230\u7070\u5ea6\u56fe\u50cf\u4e2d\u7684\u8f6e\u5ed3\u4fe1\u606f\uff0c\u5e76\u5c06\u8f6e\u5ed3\u548c\u5c42\u6b21\u7ed3\u6784\u5b58\u50a8\u5728contours\u548chierarchy\u53d8\u91cf\u4e2d cv2.RETR_LIST\u68c0\u6d4b\u7684\u8f6e\u5ed3\u4e0d\u5efa\u7acb\u7b49\u7ea7\u5173\u7cfb cv2.CHAIN_APPROX_SIMPLE\u538b\u7f29\u6c34\u5e73\u65b9\u5411\u3001\u5782\u76f4\u65b9\u5411\u3001\u5bf9\u89d2\u7ebf\u65b9\u5411\u7684\u5143\u7d20\uff0c\u53ea\u4fdd\u7559\u8be5\u65b9\u5411\u7684\u7ec8\u70b9\u5750\u6807\uff0c\u4f8b\u5982\u4e00\u4e2a\u77e9\u5f62\u8f6e\u5ed3\u53ea\u9700\u89814\u4e2a\u70b9\u6765\u4fdd\u5b58\u8f6e\u5ed3\u4fe1\u606f \uff1f cv2.findContours()\u51fd\u6570\u63a5\u53d7\u7684\u53c2\u6570\u4e3a\u4e8c\u503c\u56fe\uff0c\u5373\u9ed1\u767d\u7684\uff08\u4e0d\u662f\u7070\u5ea6\u56fe\uff09\uff0c\u6240\u4ee5\u8bfb\u53d6\u7684\u56fe\u50cf\u8981\u5148\u8f6c\u6210\u7070\u5ea6\u7684\uff0c\u518d\u8f6c\u6210\u4e8c\u503c\u56fe\u3002 6 \u8ba1\u7b97\u7b2c\u4e00\u4e2a\u8f6e\u5ed3\u7684\u8fb9\u754c\u77e9\u5f62 7 \u83b7\u53d6\u56fe\u50cf\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6 8 \u5c06\u8fb9\u754c\u6846\u7684\u5750\u6807\u4fe1\u606f\u5b58\u50a8\u5728\u5b57\u5178bbox_coords\u4e2d \u904d\u5386\u4e00\u7ec4\u56fe\u50cf\u6587\u4ef6\uff0c\u5bf9\u6bcf\u4e2a\u56fe\u50cf\u63d0\u53d6\u8fb9\u754c\u6846\u5750\u6807\u4fe1\u606f\uff0c\u5e76\u5c06\u5176\u5b58\u50a8\u5728\u5b57\u5178\u4e2d\u3002 \u83b7\u5f97\u771f\u503c\u63a9\u7801 ground_truth_masks = {} for k in bbox_coords.keys(): gt_grayscale = cv2.imread(f'ground-truth-pixel/ground-truth-pixel/{k}-px.png', cv2.IMREAD_GRAYSCALE) ground_truth_masks[k] = (gt_grayscale == 0) 1 \u4f7f\u7528imread\u4ee5\u7070\u5ea6\u56fe\u50cf\u7684\u5f62\u5f0f\u8bfb\u53d6\u6307\u5b9a\u8def\u5f84\u4e0b\u7684\u771f\u503c\u63a9\u7801\u56fe\u50cf 2 \u5c06\u5f53\u524d\u8fb9\u754c\u6846\u7684\u6807\u8bc6\u4f5c\u4e3a\u952e\uff0c\u5c06\u901a\u8fc7\u6bd4\u8f83\u771f\u503c\u63a9\u7801\u56fe\u50cf\u4e2d\u7684\u50cf\u7d20\u503c\u662f\u5426\u4e3a0\u6240\u5f97\u5230\u7684\u5e03\u5c14\u6570\u7ec4\u4f5c\u4e3a\u503c\uff0c\u5b58\u50a8\u5728ground_truth_masks\u5b57\u5178\u4e2d \u6839\u636e\u8fb9\u754c\u6846\u7684\u6807\u8bc6\uff0c\u8bfb\u53d6\u5bf9\u5e94\u7684\u771f\u503c\u63a9\u7801\u56fe\u50cf\uff0c\u5e76\u5c06\u63a9\u7801\u8f6c\u6362\u4e3a\u5e03\u5c14\u6570\u7ec4\u8868\u793a","title":"7\u9884\u5904\u7406\u6570\u636e"},{"location":"finetuneSAM/#8image-prompt-ground-truth","text":"def show_mask(mask, ax, random_color=False): if random_color: color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0) else: color = np.array([30/255, 144/255, 255/255, 0.6]) h, w = mask.shape[-2:] mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1) ax.imshow(mask_image) def show_box(box, ax): x0, y0 = box[0], box[1] w, h = box[2] - box[0], box[3] - box[1] ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2)) \u5b9a\u4e49\u4e86\u4e24\u4e2a\u51fd\u6570 show_mask() \u548c show_box()\uff0c\u7528\u4e8e\u5728Matplotlib\u4e0a\u663e\u793a\u63a9\u7801\u548c\u8fb9\u754c\u6846 name = 'stampDS-00004' image = cv2.imread(f'scans/scans/{name}.png') plt.figure(figsize=(10,10)) plt.imshow(image) show_box(bbox_coords[name], plt.gca()) show_mask(ground_truth_masks[name], plt.gca()) plt.axis('off') plt.show() ground truth mask\u975e\u5e38\u51c6\u786e\uff0c\u8fd9\u6709\u5229\u4e8e\u8ba1\u7b97\u51c6\u786e\u7684loss\u3002\u8fb9\u754c\u6846\u5c06\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u63d0\u793a\u3002","title":"8\u67e5\u770bimage prompt ground truth"},{"location":"finetuneSAM/#9","text":"model_type = 'vit_b' checkpoint = 'sam_vit_b_01ec64.pth' device = 'cuda:0' from segment_anything import SamPredictor, sam_model_registry sam_model = sam_model_registry[model_type](checkpoint=checkpoint) sam_model.to(device) sam_model.train(); \u5c06\u8f93\u5165\u56fe\u50cf\u8f6c\u6362\u4e3aSAM\u5185\u90e8\u51fd\u6570\u671f\u671b\u7684\u683c\u5f0f(\u9884\u5904\u7406\u56fe\u50cf) from collections import defaultdict import torch from segment_anything.utils.transforms import ResizeLongestSide transformed_data = defaultdict(dict) for k in bbox_coords.keys(): image = cv2.imread(f'scans/scans/{k}.png') image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) transform = ResizeLongestSide(sam_model.image_encoder.img_size) input_image = transform.apply_image(image) input_image_torch = torch.as_tensor(input_image, device=device) transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :] input_image = sam_model.preprocess(transformed_image) original_image_size = image.shape[:2] input_size = tuple(transformed_image.shape[-2:]) transformed_data[k]['image'] = input_image transformed_data[k]['input_size'] = input_size transformed_data[k]['original_image_size'] = original_image_size 1 \u8bfb\u53d6\u56fe\u50cf\u6587\u4ef6 f'scans/scans/{k}.png' \u5e76\u5b58\u50a8\u5728\u53d8\u91cf image 2 \u4f7f\u7528 cv2.cvtColor() \u5c06\u56fe\u50cf\u4ece BGR \u683c\u5f0f\u8f6c\u6362\u4e3a RGB \u683c\u5f0f 3 ResizeLongestSide\u5c06\u56fe\u50cf\u5927\u5c0f\u8c03\u6574\u4e3a sam_model.image_encoder.img_size 4 \u4f7f\u7528 torch.as_tensor() \u5c06 input_image \u8f6c\u6362\u4e3a Torch \u5f20\u91cf 5 \u4f7f\u7528 permute() \u8c03\u6574\u5f20\u91cf\u7684\u7ef4\u5ea6\u987a\u5e8f\uff0c\u5c06\u901a\u9053\u7ef4\u5ea6\u653e\u5728\u7b2c\u4e00\u7ef4\uff0c\u5e76\u4f7f\u7528 contiguous() \u4f7f\u6570\u636e\u5728\u5185\u5b58\u4e2d\u8fde\u7eed\u5b58\u50a8\uff0c\u5728\u7b2c\u4e00\u7ef4\u4e0a\u6dfb\u52a0\u4e00\u4e2a\u7ef4\u5ea6\uff0c\u5f97\u5230\u5f62\u72b6\u4e3a [1, C, H, W] \u7684\u5f20\u91cf transformed_image 6 \u4f7f\u7528 sam_model.preprocess() \u5bf9 transformed_image \u8fdb\u884c\u9884\u5904\u7406\uff0c\u5f97\u5230\u6a21\u578b\u7684\u8f93\u5165 7 \u83b7\u53d6\u539f\u59cb\u56fe\u50cf\u7684\u5c3a\u5bf8\u548c\u53d8\u6362\u540e\u56fe\u50cf\u7684\u5c3a\u5bf8\uff0c\u5e76\u5b58\u50a8\u5728 original_image_size \u548c input_size \u4e2d 8 \u5c06\u8f6c\u6362\u540e\u7684\u6570\u636e\u5b58\u50a8\u5728 transformed_data[k] \u5b57\u5178\u4e2d\uff0c\u5305\u62ec\u8f93\u5165\u56fe\u50cf input_image\u3001\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8 input_size \u548c\u539f\u59cb\u56fe\u50cf\u5c3a\u5bf8 original_image_size \u5c06\u56fe\u50cf\u6570\u636e\u8fdb\u884c\u8f6c\u6362\u548c\u9884\u5904\u7406\uff0c\u5e76\u5c06\u8f6c\u6362\u540e\u7684\u6570\u636e\u5b58\u50a8\u5728 transformed_data \u5b57\u5178\u4e2d","title":"9\u5fae\u8c03\u51c6\u5907"},{"location":"finetuneSAM/#10","text":"lr = 1e-4 wd = 0 optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=wd) loss_fn = torch.nn.MSELoss() # loss_fn = torch.nn.BCELoss() keys = list(bbox_coords.keys()) \u5b66\u4e60\u7387 \uff08\u6b65\u957f\uff09 \u6743\u91cd\u8870\u51cf \u521b\u5efa\u4e00\u4e2a Adam \u4f18\u5316\u5668\u5bf9\u8c61\u3002\u8be5\u4f18\u5316\u5668\u7528\u4e8e\u66f4\u65b0 sam_model.mask_decoder \u4e2d\u7684\u53c2\u6570 \u635f\u5931\u51fd\u6570 \u521b\u5efa\u4e00\u4e2a\u5747\u65b9\u8bef\u5dee\u635f\u5931\u51fd\u6570\u5bf9\u8c61","title":"10\u8bbe\u7f6e\u4f18\u5316\u5668"},{"location":"finetuneSAM/#11","text":"\u8981\u505a\u7684\u6539\u8fdb\u5305\u62ec\u6279\u5904\u7406\u548c\u79fb\u52a8\u56fe\u50cf\u7684\u8ba1\u7b97\uff0c\u5e76\u5728\u5faa\u73af\u4e4b\u5916\u63d0\u793a\u5d4c\u5165\uff0c\u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u8c03\u6574\u6a21\u578b\u7684\u8fd9\u4e9b\u90e8\u5206\uff0c\u8fd9\u5c06\u52a0\u5feb\u8bad\u7ec3\u901f\u5ea6\uff0c\u56e0\u4e3a\u6211\u4eec\u4e0d\u5e94\u8be5\u5728\u6bcf\u4e2a\u65f6\u671f\u91cd\u65b0\u8ba1\u7b97\u5d4c\u5165\u3002 \u6709\u65f6\u4f18\u5316\u5668\u4f1a\u8ff7\u5931\u5728\u53c2\u6570\u7a7a\u95f4\u4e2d\uff0c\u635f\u5931\u51fd\u6570\u4f1a\u5d29\u6e83\u3002 \u4ece\u5934\u5f00\u59cb\uff08\u5305\u62ec\u8fd0\u884c\u201c\u51c6\u5907\u5fae\u8c03\u201d\u4e0b\u7684\u6240\u6709\u5355\u5143\u683c\u4ee5\u4fbf\u518d\u6b21\u4f7f\u7528\u9ed8\u8ba4\u6743\u91cd\u5f00\u59cb\uff09\u5c06\u89e3\u51b3\u6b64\u95ee\u9898\u3002 from statistics import mean from tqdm import tqdm from torch.nn.functional import threshold, normalize num_epochs = 100 losses = [] for epoch in range(num_epochs): epoch_losses = [] # Just train on the first 20 examples for k in keys[:20]: input_image = transformed_data[k]['image'].to(device) input_size = transformed_data[k]['input_size'] original_image_size = transformed_data[k]['original_image_size'] # No grad here as we don't want to optimise the encoders with torch.no_grad(): image_embedding = sam_model.image_encoder(input_image) prompt_box = bbox_coords[k] box = transform.apply_boxes(prompt_box, original_image_size) box_torch = torch.as_tensor(box, dtype=torch.float, device=device) box_torch = box_torch[None, :] sparse_embeddings, dense_embeddings = sam_model.prompt_encoder( points=None, boxes=box_torch, masks=None, ) low_res_masks, iou_predictions = sam_model.mask_decoder( image_embeddings=image_embedding, image_pe=sam_model.prompt_encoder.get_dense_pe(), sparse_prompt_embeddings=sparse_embeddings, dense_prompt_embeddings=dense_embeddings, multimask_output=False, ) upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device) binary_mask = normalize(threshold(upscaled_masks, 0.0, 0)) gt_mask_resized = torch.from_numpy(np.resize(ground_truth_masks[k], (1, 1, ground_truth_masks[k].shape[0], ground_truth_masks[k].shape[1]))).to(device) gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32) loss = loss_fn(binary_mask, gt_binary_mask) optimizer.zero_grad() loss.backward() optimizer.step() epoch_losses.append(loss.item()) losses.append(epoch_losses) print(f'EPOCH: {epoch}') print(f'Mean loss: {mean(epoch_losses)}') 1 num_epochs = 100 \u8bbe\u7f6e\u4e86\u603b\u7684\u8bad\u7ec3\u8f6e\u6570 2 \u521b\u5efa\u4e00\u4e2a\u7a7a\u5217\u8868 epoch_losses\uff0c\u7528\u4e8e\u5b58\u50a8\u5f53\u524d\u8f6e\u6b21\u4e2d\u6bcf\u4e2a\u6837\u672c\u7684\u635f\u5931\u503c 3 \u4ece transformed_data \u5b57\u5178\u4e2d\u83b7\u53d6\u8f93\u5165\u56fe\u50cf input_image\u3001\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8 input_size \u548c\u539f\u59cb\u56fe\u50cf\u5c3a\u5bf8 original_image_size 4 \u4f7f\u7528 sam_model.image_encoder \u5bf9\u8f93\u5165\u56fe\u50cf\u8fdb\u884c\u7f16\u7801\uff0c\u5f97\u5230\u56fe\u50cf\u5d4c\u5165 image_embedding 5 \u4f7f\u7528 transform.apply_boxes() \u5c06\u8fb9\u754c\u6846\u4ece\u53d8\u6362\u540e\u7684\u56fe\u50cf\u5c3a\u5bf8\u8fd8\u539f\u5230\u539f\u59cb\u56fe\u50cf\u5c3a\u5bf8\uff0c\u5c06\u8fb9\u754c\u6846\u8f6c\u6362\u4e3a Torch \u5f20\u91cf 6 \u4f7f\u7528 sam_model.prompt_encoder \u5bf9\u7a00\u758f\u5d4c\u5165\u548c\u5bc6\u96c6\u5d4c\u5165\u8fdb\u884c\u7f16\u7801\uff0c\u5f97\u5230\u76f8\u5e94\u7684\u5d4c\u5165\u5f20\u91cf\u3002 7 \u4f7f\u7528 sam_model.mask_decoder \u5bf9\u56fe\u50cf\u5d4c\u5165\u548c\u5d4c\u5165\u5f20\u91cf\u8fdb\u884c\u89e3\u7801\uff0c\u5f97\u5230\u4f4e\u5206\u8fa8\u7387\u7684\u63a9\u6a21 low_res_masks \u548c IOU \u9884\u6d4b\u7ed3\u679c iou_predictions 8 \u5c06\u4f4e\u5206\u8fa8\u7387\u7684\u63a9\u6a21\u8c03\u6574\u5c3a\u5bf8\uff0c\u5e76\u5c06\u5176\u4e8c\u503c\u5316\u4e3a\u4e8c\u8fdb\u5236\u63a9\u6a21 9 \u4ece ground_truth_masks \u5b57\u5178\u4e2d\u83b7\u53d6\u76ee\u6807\u4e8c\u8fdb\u5236\u63a9\u6a21 gt_binary_mask 10 \u8ba1\u7b97loss\uff0c\u901a\u8fc7\u5c06\u4e8c\u8fdb\u5236\u63a9\u6a21\u548c\u76ee\u6807\u63a9\u6a21\u8f93\u5165\u635f\u5931\u51fd\u6570\u8fdb\u884c\u8ba1\u7b97 11 \u6e05\u9664\u4f18\u5316\u5668\u7684\u68af\u5ea6\u4fe1\u606f\u3002 12 \u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u8ba1\u7b97\u68af\u5ea6\u3002 13 \u6267\u884c\u4f18\u5316\u5668\u7684\u53c2\u6570\u66f4\u65b0\u3002 14 \u5c06\u635f\u5931\u503c\u6dfb\u52a0\u5230 epoch_losses \u5217\u8868\u4e2d mean_losses = [mean(x) for x in losses] mean_losses plt.plot(list(range(len(mean_losses))), mean_losses) plt.title('Mean epoch loss') plt.xlabel('Epoch Number') plt.ylabel('Loss') plt.show() \u4f7f\u7528\u524d\u9762\u8ba1\u7b97\u5f97\u5230\u7684 losses \u5217\u8868\u8ba1\u7b97\u4e86\u6bcf\u4e2a\u8bad\u7ec3\u8f6e\u6b21\u7684\u5e73\u5747\u635f\u5931\u503c\uff0c\u5e76\u8fdb\u884c\u4e86\u53ef\u89c6\u5316","title":"11\u8fd0\u884c\u5fae\u8c03"},{"location":"finetuneSAM/#12","text":"sam_model_orig = sam_model_registry[model_type](checkpoint=checkpoint) sam_model_orig.to(device); from segment_anything import sam_model_registry, SamPredictor predictor_tuned = SamPredictor(sam_model) predictor_original = SamPredictor(sam_model_orig) k = keys[21] image = cv2.imread(f'scans/scans/{k}.png') image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) predictor_tuned.set_image(image) predictor_original.set_image(image) input_bbox = np.array(bbox_coords[k]) masks_tuned, _, _ = predictor_tuned.predict( point_coords=None, box=input_bbox, multimask_output=False, ) masks_orig, _, _ = predictor_original.predict( point_coords=None, box=input_bbox, multimask_output=False, ) 1 \u4f7f\u7528\u8c03\u4f18\u540e\u7684\u6a21\u578b\u7684\u9884\u6d4b\u5668\u5bf9\u8c61\u5bf9\u5f85\u9884\u6d4b\u56fe\u50cf\u8fdb\u884c\u9884\u6d4b\u3002\u9884\u6d4b\u7684\u7ed3\u679c\u5b58\u50a8\u5728 masks_tuned 2 \u4f7f\u7528\u539f\u59cb\u6a21\u578b\u7684\u9884\u6d4b\u5668\u5bf9\u8c61\u5bf9\u5f85\u9884\u6d4b\u56fe\u50cf\u8fdb\u884c\u9884\u6d4b\u3002\u9884\u6d4b\u7684\u7ed3\u679c\u5b58\u50a8\u5728 masks_orig %matplotlib inline _, axs = plt.subplots(1, 2, figsize=(25, 25)) axs[0].imshow(image) show_mask(masks_tuned, axs[0]) show_box(input_bbox, axs[0]) axs[0].set_title('Mask with Tuned Model', fontsize=26) axs[0].axis('off') axs[1].imshow(image) show_mask(masks_orig, axs[1]) show_box(input_bbox, axs[1]) axs[1].set_title('Mask with Untuned Model', fontsize=26) axs[1].axis('off') plt.show() penCV-Python\u63a5\u53e3\u4e2d\u4f7f\u7528cv2.findContours()\u51fd\u6570\u6765\u67e5\u627e\u68c0\u6d4b\u7269\u4f53\u7684\u8f6e\u5ed3\u3002 contours, hierarchy = cv2.findContours(image,mode,method) image\uff1a\u8f93\u5165\u56fe\u50cf mode\uff1a\u8f6e\u5ed3\u7684\u6a21\u5f0f\u3002cv2.RETR_EXTERNAL\u53ea\u68c0\u6d4b\u5916\u8f6e\u5ed3\uff1bcv2.RETR_LIST\u68c0\u6d4b\u7684\u8f6e\u5ed3\u4e0d\u5efa\u7acb\u7b49\u7ea7\u5173\u7cfb\uff1bcv2.RETR_CCOMP\u5efa\u7acb\u4e24\u4e2a\u7b49\u7ea7\u7684\u8f6e\u5ed3\uff0c\u4e0a\u4e00\u5c42\u4e3a\u5916\u8fb9\u754c\uff0c\u5185\u5c42\u4e3a\u5185\u5b54\u7684\u8fb9\u754c\u3002\u5982\u679c\u5185\u5b54\u5185\u8fd8\u6709\u8fde\u901a\u7269\u4f53\uff0c\u5219\u8fd9\u4e2a\u7269\u4f53\u7684\u8fb9\u754c\u4e5f\u5728\u9876\u5c42\uff1bcv2.RETR_TREE\u5efa\u7acb\u4e00\u4e2a\u7b49\u7ea7\u6811\u7ed3\u6784\u7684\u8f6e\u5ed3\u3002 method\uff1a\u8f6e\u5ed3\u7684\u8fd1\u4f3c\u65b9\u6cd5\u3002cv2.CHAIN_APPROX_NOME\u5b58\u50a8\u6240\u6709\u7684\u8f6e\u5ed3\u70b9\uff0c\u76f8\u90bb\u7684\u4e24\u4e2a\u70b9\u7684\u50cf\u7d20\u4f4d\u7f6e\u5dee\u4e0d\u8d85\u8fc71\uff1bcv2.CHAIN_APPROX_SIMPLE\u538b\u7f29\u6c34\u5e73\u65b9\u5411\u3001\u5782\u76f4\u65b9\u5411\u3001\u5bf9\u89d2\u7ebf\u65b9\u5411\u7684\u5143\u7d20\uff0c\u53ea\u4fdd\u7559\u8be5\u65b9\u5411\u7684\u7ec8\u70b9\u5750\u6807\uff0c\u4f8b\u5982\u4e00\u4e2a\u77e9\u5f62\u8f6e\u5ed3\u53ea\u9700\u89814\u4e2a\u70b9\u6765\u4fdd\u5b58\u8f6e\u5ed3\u4fe1\u606f\uff1bcv2.CHAIN_APPROX_TC89_L1\uff0ccv2.CV_CHAIN_APPROX_TC89_KCOS contours\uff1a\u8fd4\u56de\u7684\u8f6e\u5ed3 hierarchy\uff1a\u6bcf\u6761\u8f6e\u5ed3\u5bf9\u5e94\u7684\u5c5e\u6027 grad\u5728\u53cd\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u662f\u7d2f\u52a0\u7684(accumulated)\uff0c\u8fd9\u610f\u5473\u7740\u6bcf\u4e00\u6b21\u8fd0\ufa08\u53cd\u5411\u4f20\u64ad\uff0c\u68af\u5ea6\u90fd\u4f1a\u7d2f\u52a0\u4e4b\u524d\u7684\u68af\u5ea6\uff0c\u6240\u4ee5\u4e00\u822c\u5728\u53cd\u5411\u4f20\u64ad\u4e4b\u524d\u9700\u628a\u68af\u5ea6\u6e05\u96f6\u3002","title":"12\u6bd4\u8f83\u5fae\u8c03\u524d\u540e\u7684\u6a21\u578b"}]}